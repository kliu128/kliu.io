<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Scenarios and Warning Signs for Ajeya's Aggressive, Conservative, and Best Guess AI Timelines | Kevin Liu</title> <meta name="author" content="Kevin Liu"/> <meta name="description" content="A summary and rephrasal of three plausible scenarios for AI timelines. Most of this post is unoriginal. Cross-posted to LessWrong, and may contain more jargon than usual."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://kliu.io/post/scenarios-and-warning-signs/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://kliu.io/">Kevin Liu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="https://www.icloud.com/sharedalbum/#B0iJtdOXmkYghC" target="_blank" rel="noopener noreferrer">photos</a> </li> <li class="nav-item "> <a class="nav-link" href="/reads">reads</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">writes</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/blog/">blog</a> <a class="dropdown-item" href="/moments/">moments</a> <a class="dropdown-item" href="/objects/">objects</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Scenarios and Warning Signs for Ajeya's Aggressive, Conservative, and Best Guess AI Timelines</h1> <p class="post-meta">March 28, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/tech"> <i class="fas fa-hashtag fa-sm"></i> tech</a>   <a href="/blog/tag/ai-safety"> <i class="fas fa-hashtag fa-sm"></i> ai-safety</a>   </p> </header> <article class="post-content"> <p><em>This post is <a href="https://www.lesswrong.com/posts/kQt3LsNAqxCh8qkaN/scenarios-and-warning-signs-for-ajeya-s-aggressive" target="_blank" rel="noopener noreferrer">cross-posted</a> to LessWrong, a rationality and AI safety community. May contain more jargon than usual.</em></p> <p>Epistemic status: mild confidence that this provides interesting discussion and debate.</p> <p>Credits to (in no particular order) Mark Xu, Sydney Von Arx, Jack Ryan, Sidney Hough, Kuhan Jeyapragasan, and Pranay Mittal for resources and feedback. Credits to Ajeya (obviously), Daniel Kokotajlo, Gwern, Robin Hanson, and many others for perspectives on timeline cruxes. This post was written as part of a 10-week AI Safety Fellowship run by Mark. All errors my own.</p> <h2 id="summary">Summary</h2> <p>Most of this post is unoriginal. It is intended primarily to summarize and rephrase the core distinctions between three plausible scenarios for AI development, which Ajeya lays out in her <a href="https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit#" target="_blank" rel="noopener noreferrer">draft report on AI timelines</a>. It also contains summaries and links to other related content.</p> <p>As a secondary goal, it attempts to lay out concrete and hopefully plausible predictions for what would occur in each of these three worlds.</p> <h2 id="glossary-from-ajeyas-report">Glossary (from Ajeya’s report)</h2> <p>This post will assume familiarity with basic terminology regarding neural networks and supervised learning.</p> <p>Before reading this post, it’s probably good to read at least a summary of Ajeya’s report (e.g. <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines?commentId=7d4q79ntst6ryaxWD" target="_blank" rel="noopener noreferrer">Rohin Shah’s</a>). Some timeline-specific terminology that is helpful to know is also listed below:</p> <ul> <li> <strong>Transformative AI (TAI)</strong>: A computer program that is as transformative as the Industrial Revolution was to the world’s trajectory.<sup id="fnref:tai-defn" role="doc-noteref"><a href="#fn:tai-defn" class="footnote" rel="footnote">1</a></sup> </li> <li> <strong>Effective horizon length (EHL)</strong>: the “length of the task”; more precisely, Ajeya defines it as the amount of data measured in subjective seconds of experience (~how long a human would take to do it) to tell whether a model has performed a task better or worse than it did previously. For some things (e.g. predict next word in the sentence), the length is quite low; for others (e.g. run a corporation that maximizes profit), the length may be quite high.</li> <li> <strong>Anchor</strong>: A concept on which to base the estimated number of FLOPS (floating-point operations/second) required to <strong>train</strong> a transformative AI. For example, you might use an Evolution Anchor, which is roughly the compute performed over all of evolution; or a Short Horizon Neural Network (NN) Anchor, which is the extrapolated compute expected for a neural network that is trained on a short horizon length. Key anchors are: <ul> <li> <strong>Lifetime anchor</strong>: the amount of compute performed by “one human brain over one lifetime.”</li> <li> <strong>Short/medium/long-horizon NN</strong>: the compute required to train a NN of a size anchored to the human brain on short, medium, or long-horizon tasks, respectively.</li> <li> <strong>Evolution anchor</strong>: the amount of compute performed over all of evolution.</li> </ul> </li> </ul> <h2 id="scenario-one">Scenario One</h2> <p>If you believe…</p> <h3 id="short-horizon-nn-has-a-fair-chance-of-succeeding-eg-40">Short-horizon NN has a fair chance of succeeding (e.g. 40%)<sup id="fnref:hypotheses-caveat" role="doc-noteref"><a href="#fn:hypotheses-caveat" class="footnote" rel="footnote">2</a></sup> </h3> <p>You might believe this if you think there’s a good chance it is sufficient to fine-tune a large language model like GPT-N for TAI, and there’s no need to train models directly on tasks that take a long time to judge (e.g. “writing a twist ending to a story”, as opposed to “predict next word”). Concrete things you might expect to happen soon would thus be:</p> <ol> <li>GPT-4+ can be fine-tuned to perform longer-horizon tasks, such as writing long and coherent stories, with less compute cost than the original pretraining.</li> <li>This capability to generalize improves as the pre-training process improves (e.g. GPT-5 is much better at fine-tuning than GPT-4, which is much better than GPT-3). <a href="https://www.alignmentforum.org/posts/pqkdsqd6s6w2HtT9g/intermittent-distillations-1#Scaling_Laws_for_Transfer__Danny_Hernandez_" target="_blank" rel="noopener noreferrer">The scaling laws of model generalization</a> perhaps hold up for much larger models.</li> <li>By 2030, short-horizon models have achieved at least partial meta-learning. For example, an RL model can learn to play novel video games as well as a human can after a few hours of practice. (This criterion comes from a Q&amp;A with Ajeya, although the timeline estimate is my own.)</li> </ol> <p>For more reading, see <a href="https://docs.google.com/document/d/1PaYOh_9BAYEm3RfpeX0G-cvs5JxGns98IsVK061jqRQ/edit#heading=h.f8rh9g7pqv91" target="_blank" rel="noopener noreferrer">Ajeya on downstream skills</a>.</p> <h3 id="algorithms-halve-compute-requirements-every-2-years-for-short-horizon-nns-or-every-1-year-for-a-mediumlong-horizon-nn">Algorithms halve compute requirements every ~2 years for short-horizon NNs, or every ~1 year for a medium/long-horizon NN.</h3> <p>You might expect this if you think there is a lot of “low-hanging fruit” in algorithms, such as if you think relatively little work has gone into optimizing training regimes or architectures for large NNs. (For context, OpenAI’s <a href="https://openai.com/blog/ai-and-efficiency/" target="_blank" rel="noopener noreferrer">AI &amp; Efficiency</a> suggests a halving time of ~16 months for ImageNet models.) Consequences you might expect are:</p> <ol> <li>An increase in companies working to improve massive (&gt;100bn parameter) language models. For example, you might expect at least 5 large tech companies working on algorithms by 2025, as opposed to “mostly only OpenAI &amp; <a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener noreferrer">Google</a> in 2020”.<sup id="fnref:gwern-is-cool" role="doc-noteref"><a href="#fn:gwern-is-cool" class="footnote" rel="footnote">3</a></sup> Some evidence possibly in favor of this is that Alibaba + Tsinghua University recently released (Feb 2021) <a href="https://arxiv.org/pdf/2103.00823.pdf" target="_blank" rel="noopener noreferrer">M6: A Chinese Multimodal Pretrainer</a> with 100 billion parameters (although this uses a Mixture of Experts model whose efficacy I am unfamiliar with).</li> <li>Multiple significant (e.g. 4x+) speedups targeted specifically toward training large language models by 2030. (This implies that there is significant low-hanging fruit being taken.)</li> </ol> <h3 id="moores-law-returns-resulting-in-15-year-doubling-times-for-flops-to-train-a-model">Moore’s Law returns, resulting in ~1.5 year doubling times for FLOPs/$ to train a model.</h3> <p>For context, Moore’s law described transistor progress well until the mid-2000s, when the regime shifted to a doubling time of ~3-4 years<sup id="fnref:moore" role="doc-noteref"><a href="#fn:moore" class="footnote" rel="footnote">4</a></sup>. One possible story for a return to ~1.5-year doubling times is that the number of chip producers increases, with players perhaps aided by AI-assisted chip manufacturing. Moore’s Law is rather hard to predict, but concrete things that <em>might</em> allow this are:</p> <ol> <li> <strong>Arms race</strong>: <a href="https://www.bloomberg.com/news/articles/2021-02-11/europe-weighs-semiconductor-foundry-to-fix-supply-chain-risk" target="_blank" rel="noopener noreferrer">The European Union makes a competitive semiconductor manufacturing factory</a>. Semiconductor manufacturing becomes more politicized, encouraging an “arms race” of sorts. One reason this might happen is if <a href="https://www.theguardian.com/world/2021/mar/10/china-could-invade-taiwan-in-next-six-years-top-us-admiral-warns" target="_blank" rel="noopener noreferrer">China invades Taiwan</a>, possibly taking over major semiconductor manufacturing company TSMC.</li> <li> <strong>Competition</strong>: <a href="https://seekingalpha.com/article/4392173-tsmc-to-fall-behind-intel-samsung-2024" target="_blank" rel="noopener noreferrer">Intel</a> and <a href="https://www.bloomberg.com/news/articles/2020-11-17/samsung-intensifies-chip-wars-with-bet-it-can-catch-tsmc-by-2022" target="_blank" rel="noopener noreferrer">Samsung</a> catch up with TSMC for chip manufacturing. There are several chip manufacturers in direct competition at the cutting edge by 2025.</li> <li> <strong>AI-assisted chip fabrication</strong>: E.g. a 10x decrease in cost by 2030 due to ML systems that make various parts of production more efficient. Current examples include <a href="https://arstechnica.com/information-technology/2019/06/manufacturing-memory-means-scribing-silicon-in-a-sea-of-sensors/?comments=1" target="_blank" rel="noopener noreferrer">listening for defects in production</a>, <a href="https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html" target="_blank" rel="noopener noreferrer">chip placement with RL</a>, <a href="https://ai.googleblog.com/2021/02/machine-learning-for-computer.html" target="_blank" rel="noopener noreferrer">optimizing chip architectures for a given model</a>, or <a href="https://www.edn.com/machine-learning-in-eda-accelerates-the-design-cycle/" target="_blank" rel="noopener noreferrer">electronic design automation</a>.</li> </ol> <h3 id="ai-companies-are-rapidly-willing-to-spend-2-of-gdp-to-train-a-transformative-model">AI companies are rapidly willing to spend 2% of GDP to train a transformative model.</h3> <p>This looks like AI companies rapidly realizing the immense value of training bigger AI, so they rapidly scale up until they are limited by capital-on-hand in 2030. From that point on, they are willing to spend 2% of GDP on training a model. You might expect this if:</p> <ol> <li>AI shows concrete economic benefits in the short term. Commercialization of GPT models earns $&gt;1bn in revenue by 2025<sup id="fnref:hansonbet" role="doc-noteref"><a href="#fn:hansonbet" class="footnote" rel="footnote">5</a></sup>, with customers willing to pay for the outputs of fine-tuned models or more polished AI-assisted GPT products.</li> <li>Big tech companies like Google realize the profit opportunity and begin to quickly scale up models (and pricing models) of their own. One concrete example is “Google makes DeepMind build a GPT-N competitor, which it contracts to governments or other institutions.”</li> <li>By 2025, the US or similar national government experiments with training a large language model. (This implies government involvement in AI, which could dramatically increase funding.)</li> </ol> <p><strong>Given the above, you should expect a median timeline of 2036, shaded up by Ajeya to 2040.</strong></p> <h2 id="scenario-two">Scenario Two</h2> <h3 id="nns-likely-require-some-medium--to-long-horizon-training">NNs likely require some medium- to long-horizon training.</h3> <p>You might believe this if you think scaling up GPT-3 doesn’t quite lead to TAI. It gets you a good bit of the way there, but turns out you need a more complex environment to learn <em>how to learn</em> new tasks. This might look like:</p> <ol> <li> <p><strong>Fine-tuning hits a wall</strong>: By 2025, it’s clear that massive fine-tuned language models <em>underperform</em> in comparison to smaller models that use supervised learning directly on the question at hand (e.g. large-scale code generation). Concretely, a possible scenario is that GPT-5-CodeCompletionXL performs worse than a new TransformerCodeM model, which was trained directly on sets of code completion questions rather than unsupervised learning like GPT.</p> <p>This implies that new advances will be required to reach scalable TAI.</p> <p>(<a href="https://docs.google.com/document/d/1PaYOh_9BAYEm3RfpeX0G-cvs5JxGns98IsVK061jqRQ/edit#heading=h.f8rh9g7pqv91" target="_blank" rel="noopener noreferrer">Ajeya mentions</a> some reasons why you might expect a from-scratch supervised model to outperform a fine-tuned language model, but whether it would do so in reality is an open question.)</p> </li> <li> <p><strong>Training on long horizons becomes popular</strong>: By 2030, there exist 2+ models achieving state-of-the-art performance in a specific field that use training on long horizons (greater than few minutes). For example, a novel-writing AI that receives feedback only after finishing a novel, or an RL agent that plays long and complex games.</p> </li> </ol> <h3 id="algorithmic-progress-is-a-bit-slower-than-it-was-in-the-past-halving-compute-every-3-years-for-short-horizon-nns-2-years-for-medium--long-horizon-nns">Algorithmic progress is a bit slower than it was in the past, halving compute every 3 years for short-horizon NNs, 2 years for medium &amp; long-horizon NNs.</h3> <p>You might believe this if you think most of the low-hanging fruit has been picked. Architectural advancements slow down, with each one representing mostly incremental progress.</p> <p>(For further reading and intuitions, AI Impacts has a page with <a href="https://aiimpacts.org/trends-in-algorithmic-progress/" target="_blank" rel="noopener noreferrer">many examples of past algorithmic progress</a>. See also the posts mentioned in Ajeya’s paper: <a href="https://arxiv.org/pdf/2005.04305.pdf" target="_blank" rel="noopener noreferrer">Measuring the Algorithmic Efficacy of Neural Networks (2020)</a> and <a href="https://intelligence.org/files/AlgorithmicProgress.pdf" target="_blank" rel="noopener noreferrer">Algorithmic Progress in Six Domains (2013)</a>.)</p> <p>Things you may expect are:</p> <ol> <li>By 2025, the number of companies working on large language models has plateaued or even declined. E.g. <a href="https://www.gwern.net/newsletter/2020/05#gpt-3" target="_blank" rel="noopener noreferrer">Google Brain and DeepMind still refuse to buy into the scaling hypothesis</a>; large language model startups (e.g. <a href="https://cohere.ai/mlrs" target="_blank" rel="noopener noreferrer">Cohere</a>) have flopped.</li> <li>By 2025, the large size and training time of models becomes a bottleneck to experimentation. For example, if it takes weeks and significant compute resources to run an experiment to improve efficiency, then testing out new improvements becomes much slower. We see some hints of this in the training of <a href="https://arxiv.org/pdf/1912.06680.pdf" target="_blank" rel="noopener noreferrer">OpenAI Five (“surgery”)</a>, although it’s unclear to me how much of a time penalty this adds, or if this problem can be avoided by using smaller models to experiment with improvements rather than bigger ones.</li> </ol> <h3 id="moores-law-slows-a-bit-flops-doubles-every-25-years">Moore’s Law slows a bit. FLOPS/$ doubles every 2.5 years.</h3> <p>By 2025, the general consensus is that Moore’s Law is dead. TSMC, Intel, and Samsung hit manufacturing delays in new nodes, and it is projected that doubling time will increase. There is a solid path for further growth, but the path forward is hard, and chip designers focus more on optimizing preexisting nodes much like Intel does today.</p> <p>Things you might expect are:</p> <ol> <li>By 2025, Intel, Samsung, and TSMC all <a href="https://seekingalpha.com/article/4392173-tsmc-to-fall-behind-intel-samsung-2024" target="_blank" rel="noopener noreferrer">fall behind on their cadences</a>. Delays, like those that have <a href="https://www.bbc.com/news/technology-53525710" target="_blank" rel="noopener noreferrer">plagued Intel</a>, spread to the entire industry.</li> <li>Competition remains slim. For example, current export restrictions on semiconductors<sup id="fnref:more-than-you-wanted-to-know-about-semiconductors" role="doc-noteref"><a href="#fn:more-than-you-wanted-to-know-about-semiconductors" class="footnote" rel="footnote">6</a></sup> are successful at limiting China’s semiconductor fabrication <em>without triggering an arms race</em>, preventing additional competition from arising.</li> </ol> <h3 id="ai-companies-are-willing-to-spend-1bn-in-2025-with-that-figure-doubling-every-2-years">AI companies are willing to spend $1bn in 2025, with that figure doubling every 2 years.</h3> <p>Ajeya considers this a plausible level of spending on a “business as usual” trajectory, given the current market cap and cash on hand of major tech companies. See <a href="https://docs.google.com/document/d/1qjgBkoHO_kDuUYqy_Vws0fpf-dG5pTU4b8Uej6ff2Fg/edit#" target="_blank" rel="noopener noreferrer">here</a> for more details.</p> <p><strong>Given the above, you should expect a median timeline of ~2052.</strong></p> <h2 id="scenario-three">Scenario Three</h2> <h3 id="transformative-nns-likely-depend-highly-on-long-horizon-training-perhaps-requiring-flops-on-the-order-of-evolutionary-computation">Transformative NNs likely depend highly on long-horizon training, perhaps requiring FLOPs on the order of evolutionary computation.</h3> <p>This looks like GPT &amp; supervised learning hitting a dead-end for meta-learning (learning new, complex tasks). No matter how hard we try, we can’t get neural networks to learn complex skills over short training timeframes. One of the options left to us is something like <a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#1__Reinforcement_learning___transparency_tools" target="_blank" rel="noopener noreferrer">RL + transparency tools</a> or supervised learning with feedback that is given only after long subjective time elapsed, which are both highly compute-intensive ways of training an agent.</p> <p>Things you might expect are:</p> <ol> <li>By 2030, spending on massive LMs plateaus such that there is &lt;1 doubling in 3 years. The general consensus is that large language models are powerful, but yield diminishing returns, and are significantly limited at tasks that take a human more than 5 minutes to consider.</li> <li>“AI winter;” qualitatively, advances comparable to significant advances of the past five years (e.g. AlphaGo, GPT-2, GPT-3) are much fewer and further between as low-hanging fruit is already picked. This period may last for 10+ years.</li> </ol> <h3 id="algorithms-halve-compute-every-4-years-for-short-horizon-3-years-for-medium-and-long-horizon">Algorithms halve compute every 4 years for short-horizon, 3 years for medium and long horizon.</h3> <p>This looks a lot like Scenario Two, but is quantitatively a bit slower. Realistically, the biggest sign of this is probably just a slowing trend in 2025’s “algorithmic progress” chart, but other things you might expect are:</p> <ol> <li>A decline in discoveries of new neural network architectures and techniques for efficiency. If you were to plot the major discoveries of a field such as NLP on a graph, you would see a flurry of discoveries in the 2010s, but by the end of the 2020s progress has significantly slowed.</li> </ol> <h3 id="moores-law-slows-significantly-doubling-flops-every-35-years">Moore’s law slows significantly, doubling FLOPS/$ every 3.5 years.</h3> <p>This might occur if <a href="https://www.extremetech.com/computing/272096-3nm-process-node" target="_blank" rel="noopener noreferrer">silicon process costs keep rising</a>, eventually becoming uneconomical even for large players. There are incremental further advancements (e.g. optimizing 3nm+++), but overall stagnation continues.</p> <p>Things you might expect are:</p> <ol> <li>By 2030, all major chip producers (e.g. Intel, AMD, Apple, NVIDIA) have significantly increased their timelines for moving to new nodes. For example, if on 3nm, they plan to move to 1nm in 3+ years.</li> <li>More unlikely: a <a href="https://www.gwern.net/Slowing-Moores-Law" target="_blank" rel="noopener noreferrer">state-sponsored effort to slow down chip fabrication</a>, perhaps due to global war and instability.</li> </ol> <h3 id="ai-companies-will-have-to-wait-for-the-entire-economy-to-grow-sufficiently-to-finance-tai">AI companies will have to wait for the entire economy to grow sufficiently to finance TAI.<sup id="fnref:caveat-spending" role="doc-noteref"><a href="#fn:caveat-spending" class="footnote" rel="footnote">7</a></sup> </h3> <p>This follows from the slowing of Moore’s Law and the need for expensive, long-horizon training. This world seems like one where advanced AI is not terribly profitable and requires resources on the scale of a 2090s <a href="https://en.wikipedia.org/wiki/Megaproject" target="_blank" rel="noopener noreferrer">megaproject</a>.</p> <p>Things you might expect are:</p> <ol> <li>Efforts to commercialize large language models are not profitable. OpenAI shuts down the GPT-3 API by 2025, opting for a different business model. Further endeavors reach only limited profitability due to limited use cases.</li> <li>“AI winter,” as discussed in the first section, causes a lack of investment in AI companies. One possible world is that spending in AI <a href="https://docs.google.com/document/d/1cCJjzZaJ7ATbq8N2fvhmsDOUWdm7t3uSSXv6bD0E_GM/edit#heading=h.x0pkk2mc19ey" target="_blank" rel="noopener noreferrer">plateaus for several decades</a>, before experiencing another period of exponential growth as a new paradigm is discovered in the late 21st century.</li> </ol> <p><strong>Given the above, you should expect a median timeline of **2100</strong>, shaded down to 2090 by Ajeya.**</p> <h1 id="epistemic-notes">Epistemic Notes</h1> <p>The above predictions are obviously rough. Even in a world that satisfies a particular timeline (e.g. AI by 2052), I expect the specifics of more than half of them to probably be wrong. However, the hope is that these predictions can be used as a sort of barometer, so that five years down the line, we can look back and ask, “how many of these came true?” The answer may help us figure out when we predict TAI to eventually arrive.</p> <p>I also hope these predictions can be used today to clarify researchers’ own timelines. If you believe most of the predictions in Scenario 1 are plausible, for example, you may want to update toward shorter timelines, and likewise if you think Scenario 3 is plausible, you should probably update toward later timelines.</p> <h1 id="other-scenarios-and-open-questions">Other Scenarios and Open Questions</h1> <p>In the process of writing this and further understanding Ajeya’s assumptions, I had a few ideas for other scenarios that I would enjoy seeing fleshed out.</p> <ul> <li>What happens if government spending gets involved? What are reasonable estimates for spending?</li> <li>How likely is it that tasks such as meta-learning can be solved via “secret sauce” waiting to be discovered, rather than pure algorithmic brute force?</li> <li>How do you make better predictions about Moore’s Law?</li> <li> <a href="https://www.gwern.net/newsletter/2020/05#fn19" target="_blank" rel="noopener noreferrer">[via Gwern]</a>: How will a 1T-parameter language model behave? 10T? 100T?</li> <li>How will multimodal models with natural language supervision (e.g. <a href="https://openai.com/blog/clip/" target="_blank" rel="noopener noreferrer">CLIP</a>, <a href="https://arxiv.org/pdf/2103.00823.pdf" target="_blank" rel="noopener noreferrer">M6</a>) affect algorithmic progress? Could multimodal models open a “greenfield” of low-hanging algorithmic fruit?</li> <li>What do “counterarguments to each scenario” look like? What compelling current or future evidence would cause you to reject a scenario?</li> </ul> <h1 id="further-reading--related-work">Further Reading &amp; Related Work</h1> <p>Here are some of the documents I found useful while writing this post.</p> <ul> <li><a href="https://forum.effectivealtruism.org/posts/QAqghTmp7FSMcJ4ch/ama-ajeya-cotra-researcher-at-open-phil?commentId=KnidjKuibKyrDunbZ#KnidjKuibKyrDunbZ" target="_blank" rel="noopener noreferrer">Ajeya’s comments on the types of disagreements with AI timelines</a></li> <li><a href="https://fas.org/sgp/crs/misc/R46581.pdf" target="_blank" rel="noopener noreferrer">Semiconductors and Federal Policy</a></li> <li>For context on how quickly we expect AI to gain economic value, I found <a href="https://forum.effectivealtruism.org/posts/QAqghTmp7FSMcJ4ch/ama-ajeya-cotra-researcher-at-open-phil?commentId=HGu4qZx5E93JbuFDb#KnidjKuibKyrDunbZ" target="_blank" rel="noopener noreferrer">Ajeya and Gwern’s discussion on the “neglectedness” of AI solutions</a> insightful.</li> <li>Daniel Kokotajlo’s <a href="https://www.lesswrong.com/posts/rzqACeBGycZtqCfaX" target="_blank" rel="noopener noreferrer">Fun with +12 OOMs of Compute</a> describes a thought experiment explaining how you might envision using $10^{35}$ FLOPs of compute.</li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:tai-defn" role="doc-endnote"> <p>See https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit#heading=h.6t4rel10jbcj <a href="#fnref:tai-defn" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:hypotheses-caveat" role="doc-endnote"> <p>This crux simplifies Ajeya’s <a href="https://docs.google.com/document/d/1cCJjzZaJ7ATbq8N2fvhmsDOUWdm7t3uSSXv6bD0E_GM/edit#" target="_blank" rel="noopener noreferrer">range of estimates for hypothesis probabilities</a> considerably. For each scenario, I’ve used the most probable anchor as the “central crux”, as it seems to me that if you believe short-horizon NN is 40% likely to work, then the other hypotheses are also fairly sensible. <a href="#fnref:hypotheses-caveat" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:gwern-is-cool" role="doc-endnote"> <p>This is reminiscent of Gwern’s <a href="https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang?commentId=jbD8siv7GMWxRro43" target="_blank" rel="noopener noreferrer">earlier post</a> from “Are we in an AI overhang?” <a href="#fnref:gwern-is-cool" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:moore" role="doc-endnote"> <p>Sourced from https://docs.google.com/document/d/1cCJjzZaJ7ATbq8N2fvhmsDOUWdm7t3uSSXv6bD0E_GM/edit#heading=h.96w8mskhfp5l <a href="#fnref:moore" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:hansonbet" role="doc-endnote"> <p>Credits to Robin Hanson’s bet here: https://twitter.com/robinhanson/status/1297325331158913025 <a href="#fnref:hansonbet" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:more-than-you-wanted-to-know-about-semiconductors" role="doc-endnote"> <p>The US has recently <a href="https://www.tomshardware.com/news/smic-import-restrictions" target="_blank" rel="noopener noreferrer">prevented Chinese companies such as SMIC</a> from getting high-end manufacturing equipment. It also has <a href="https://www.brookings.edu/techstream/the-chip-making-machine-at-the-center-of-chinese-dual-use-concerns/" target="_blank" rel="noopener noreferrer">worked with the Dutch</a> to deny ASML, a machine manufacturing company, from delivering high-quality EUV chip machines to China. Quick research seems to suggest that <a href="https://semiwiki.com/semiconductor-services/semiconductor-advisors/281384-asml-euv-china-chip-equip-risk/" target="_blank" rel="noopener noreferrer">building a manufacturing plant without US equipment</a> is extremely hard, at least for now. <a href="#fnref:more-than-you-wanted-to-know-about-semiconductors" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:caveat-spending" role="doc-endnote"> <p>This is derived from Ajeya’s assumption that spending will start at 300 million dollars in 2025 and grow every 3 years, reaching a max of 100bn by 2055, at which point it is bounded at 0.5% of GDP. However, since TAI isn’t projected until 2100 in this scenario anyway, in practice this means that only the upper bound really matters. <a href="#fnref:caveat-spending" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </article> <hr> <div style="text-align: center;"> <p>If you like reading these posts, get new ones via email:</p> <div id="mc_embed_signup"> <form action="https://kliu.us10.list-manage.com/subscribe/post?u=5539e4dff5ee082eef001e612&amp;id=f9f0fa279e&amp;f_id=000b36e2f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate> <div id="mc_embed_signup_scroll"> <div class="mc-field-group" style="margin-bottom: 1em"> <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required placeholder="email address"> <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span> <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"> </div> <div id="mce-responses" class="clear foot"> <div class="response" id="mce-error-response" style="display:none"></div> <div class="response" id="mce-success-response" style="display:none; margin-bottom: 1em"></div> </div> <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_5539e4dff5ee082eef001e612_f9f0fa279e" tabindex="-1" value=""></div> <div class="optionalParent"> <div class="clear foot"> </div> </div> </div> </form> </div> <script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script> <script type="text/javascript">jQuery,window.fnames=new Array,window.ftypes=new Array,fnames[0]="EMAIL",ftypes[0]="email",fnames[1]="FNAME",ftypes[1]="text",fnames[2]="LNAME",ftypes[2]="text",fnames[3]="ADDRESS",ftypes[3]="address",fnames[4]="PHONE",ftypes[4]="phone",fnames[5]="BIRTHDAY",ftypes[5]="birthday";var $mcj=jQuery.noConflict(!0);</script> </div> <div id="disqus_thread"></div> <script type="text/javascript">var disqus_shortname="kevin-liu",disqus_identifier="/post/scenarios-and-warning-signs",disqus_title="Scenarios and Warning Signs for Ajeya's Aggressive, Conservative, and Best Guess AI Timelines";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © 2022 Kevin Liu. Licensed under CC BY-SA 4.0. Last updated: October 23, 2022. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KKF0HL5TKT"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-KKF0HL5TKT");</script> </body> </html>