<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>LizardFS is Pretty Nice | Kevin Liu</title> <meta name="author" content="Kevin Liu"/> <meta name="description" content="My LizardFS storage setup - basically living the dream"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://kliu.io/post/lizardfs-is-pretty-nice/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://kliu.io/">Kevin Liu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="https://www.icloud.com/sharedalbum/#B0iJtdOXmkYghC" target="_blank" rel="noopener noreferrer">photos</a> </li> <li class="nav-item "> <a class="nav-link" href="/reads">reads</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">writes</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/blog/">blog</a> <a class="dropdown-item" href="/moments/">moments</a> <a class="dropdown-item" href="/objects/">objects</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">LizardFS is Pretty Nice</h1> <p class="post-meta">January 21, 2019</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/tech"> <i class="fas fa-hashtag fa-sm"></i> tech</a>   <a href="/blog/tag/homelab"> <i class="fas fa-hashtag fa-sm"></i> homelab</a>   </p> </header> <article class="post-content"> <p>For the past few months, I’ve been running <a href="https://lizardfs.com/" target="_blank" rel="noopener noreferrer">LizardFS</a> on my home servers, providing 26 TB of error-checked, erasure-coded storage. All on mismatched disks spread over two computers.</p> <p>I like it. A lot.</p> <h1 id="the-problem">The Problem</h1> <p>As an unorganized digital packrat, I have spontaneously purchased hard drives of very varying capacities over the years. In my storage cluster, I have 250 GB disks all the way to 8 TB disks. Conventional RAID filesystems, like ZFS or software RAID, tend to not handle mismatched disks very well, and they do not let you use all the disks to their fullest capacity.</p> <p>With ZFS especially, it requires <a href="https://www.reddit.com/r/zfs/comments/85nf1y/zfs_with_different_size_disks/" target="_blank" rel="noopener noreferrer">quite some finangling</a> to create a storage pool from mismatched disks, and adding new disks to a RAIDZ is impossible without <a href="https://louwrentius.com/the-hidden-cost-of-using-zfs-for-your-home-nas.html" target="_blank" rel="noopener noreferrer">creating more vdevs</a>, which incurs parity overhead. Thus, despite ZFS’s legendary other features (e.g. deduplication, snapshotting, block devices…), it wouldn’t work for me.</p> <p>BTRFS is also another option, but, uh, <a href="https://btrfs.wiki.kernel.org/index.php/RAID56" target="_blank" rel="noopener noreferrer">yeah</a>. Let’s just say, I wouldn’t trust BTRFS on anything larger than a single-disk filesystem. (Heck, even my root filesystem on BTRFS crashes every few weeks with some arcane BTRFS error.)</p> <p>An option that is <em>nearly</em> ideal is <a href="http://www.snapraid.it/" target="_blank" rel="noopener noreferrer">SnapRAID</a>. With SnapRAID, you simply put files on individual disks, and SnapRAID periodically (e.g. daily) reads through the disks and calculates parity, which it puts on a <strong>parity drive</strong>. The downside, however, is that (1) this delayed parity calculation means new files are at risk for some time, and (2) the parity drive must be the largest of all your drives. In my case, this would mean giving an entire 8 TB drive to parity, which is not really ideal. And if I were to want double parity? Two 8 TB drives.</p> <p>Also, a shared issue with these filesystems is that they are inherently <em>single-computer</em> – there’s no sharing of a ZFS filesystem across multiple computers, or calculating SnapRAID parity across two PCs.</p> <h1 id="lizardfs-the-solution">LizardFS: The Solution</h1> <p>LizardFS operates in a similar manner to SnapRAID – it writes directly to ordinary filesystems on individual drives (e.g. ext4, BTRFS). It essentially provides a virtual filesystem that you can write to, and it will pick drives to write to. However, it doesn’t write files whole – it breaks each into 64 MB chunks and distributes them across its drives. This means it can use all disks to their fullest capacity.</p> <p>To provide redundancy, however, it also provides <strong>goals</strong>, different options for how data should be replicated/erasure-coded. Goals can be set on individual files or folder trees, so you could have your movie collection at 2 data : 1 parity (so one drive could fail before losing data), while your personal documents are at 2x replication.</p> <p>To manage this entire system, LizardFS operates a <strong>master daemon</strong>, which handles all filesystem metadata and lets clients know where to find chunks. Chunks are served and written by <strong>chunkservers</strong>, which deal directly with individual disks. These chunkservers can be on different computers, providing redundancy and scalability. There are two ways to think about chunkservers:</p> <ol> <li>One chunkserver per computer; the chunkserver writes to a ZFS array/RAID array/etc.</li> <li> <a href="https://github.com/lizardfs/lizardfs/issues/231" target="_blank" rel="noopener noreferrer">One chunkserver per disk</a>; the chunkserver writes to an individual disk.</li> </ol> <p>I went for the latter, because I am too poor for option one, and I only have two computers. (The benefit of option one, though, is that you can <em>literally unplug a computer</em> and your file system is fine.)</p> <p>LizardFS also provides snapshot functionality, though I personally can’t use them (see below).</p> <p>This kind of architecture is similar to those of other distributed filesystems, including Ceph and GlusterFS. As far as I know, though, neither has as much flexibility in, e.g. assigning different goals to files.</p> <h1 id="how-i-run-lizardfs">How I Run LizardFS</h1> <p>I run LizardFS on two computers. One (i7-4820k, 20 GB RAM) runs the <strong>master daemon</strong> and chunkservers, while the other (AMD Athlon X2, 6 GB RAM) runs more chunkservers and the <strong>metalogger</strong> (basically a backup for the master daemon).</p> <p>I orchestrate this entire process using <a href="https://nixos.org/" target="_blank" rel="noopener noreferrer">NixOS</a>, <a href="https://nixos.org/nixops/" target="_blank" rel="noopener noreferrer">NixOps</a>, and a custom LizardFS configuration module that I have written.</p> <p>Most of my files are at <code class="language-plaintext highlighter-rouge">ec(2,1)</code>; this means that I can lose one drive and still reconstruct all data. Important files are at <code class="language-plaintext highlighter-rouge">ec(4,2)</code>, so I can lose 2 drives for those.</p> <h1 id="the-downsides">The Downsides</h1> <p>A few things about LizardFS make me worried about its longer-term relevance.</p> <ol> <li> <p><a href="https://github.com/lizardfs/lizardfs" target="_blank" rel="noopener noreferrer">The repository</a> hasn’t been updated since June 2018. Now, it seems that they only update their public repository on new software releases, but the fact remains that v3.13.0-rc1 has been out for nearly a year (and rather <a href="https://github.com/lizardfs/lizardfs/issues/746" target="_blank" rel="noopener noreferrer">buggily at that</a>) with no stable release.</p> </li> <li> <p>There have been some bugs. Sometimes, after my computer crashes, taking down the master daemon and several chunkservers, I reboot to find missing chunks. I believe this is because mounts don’t wait for all data + EC writes to finish, which may be solved by <a href="https://github.com/lizardfs/lizardfs/issues/338" target="_blank" rel="noopener noreferrer">the <code class="language-plaintext highlighter-rouge">REDUNDANCY_LEVEL</code> option</a>. I’m still worried that it can happen <em>at all</em>, though.</p> </li> <li> <p>Scalability. The master daemon stores all metadata (e.g. filenames, modtimes) in RAM, providing lightning-quick access, but at the cost of memory usage. At times, it has grown to ~1.5 GB. This also makes extensive snapshots unusable for me, as each snapshot essentially <em>duplicates the entire metadata set</em>, doubling memory usage.</p> <p>The daemon also <a href="https://github.com/lizardfs/lizardfs/issues/323" target="_blank" rel="noopener noreferrer">forks on the hour</a>, which briefly doubles memory usage. So you basically have to reserve twice the size of the metadata set in RAM.</p> <p>I’ve tried to work around this by forcing the daemon to use swap by setting <a href="https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">MemoryMax=</code> in <code class="language-plaintext highlighter-rouge">systemd</code></a>. This works surprisingly well (metadata access is still pretty fast), but it causes several-minute hangs on shutdown, so I stopped doing it.</p> </li> <li> <p>Not a lot of people use it. While Ceph / GlusterFS / SnapRAID have lots of blog posts on the internet and setup guides, LizardFS has quite sparse documentation. Hence why I’m writing this post now. For more info, I would recommend <a href="https://www.reddit.com/r/HomeServer/comments/98ex85/one_server_isnt_enough_my_adventures_with_lizardfs/" target="_blank" rel="noopener noreferrer">wintersdark’s guide on reddit</a>, which introduced me to LizardFS.</p> <p>If anyone else is using LizardFS or has any questions about trying it out, please <a href="/about">contact me</a>!</p> </li> </ol> <p>Overall, though, these aren’t too bad. It’s the best storage solution available right now for my use case, and I’m happy &lt;3</p> </article> <hr> <div style="text-align: center;"> <p>If you like reading these posts, get new ones via email:</p> <div id="mc_embed_signup"> <form action="https://kliu.us10.list-manage.com/subscribe/post?u=5539e4dff5ee082eef001e612&amp;id=f9f0fa279e&amp;f_id=000b36e2f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate> <div id="mc_embed_signup_scroll"> <div class="mc-field-group" style="margin-bottom: 1em"> <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required placeholder="email address"> <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span> <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"> </div> <div id="mce-responses" class="clear foot"> <div class="response" id="mce-error-response" style="display:none"></div> <div class="response" id="mce-success-response" style="display:none; margin-bottom: 1em"></div> </div> <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_5539e4dff5ee082eef001e612_f9f0fa279e" tabindex="-1" value=""></div> <div class="optionalParent"> <div class="clear foot"> </div> </div> </div> </form> </div> <script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script> <script type="text/javascript">jQuery,window.fnames=new Array,window.ftypes=new Array,fnames[0]="EMAIL",ftypes[0]="email",fnames[1]="FNAME",ftypes[1]="text",fnames[2]="LNAME",ftypes[2]="text",fnames[3]="ADDRESS",ftypes[3]="address",fnames[4]="PHONE",ftypes[4]="phone",fnames[5]="BIRTHDAY",ftypes[5]="birthday";var $mcj=jQuery.noConflict(!0);</script> </div> <div id="disqus_thread"></div> <script type="text/javascript">var disqus_shortname="kevin-liu",disqus_identifier="/post/lizardfs-is-pretty-nice",disqus_title="LizardFS is Pretty Nice";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © 2022 Kevin Liu. Licensed under CC BY-SA 4.0. Last updated: October 23, 2022. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KKF0HL5TKT"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-KKF0HL5TKT");</script> </body> </html>